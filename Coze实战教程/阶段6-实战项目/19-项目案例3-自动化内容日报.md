## **第十九节 · AI 生成质量评估与反馈闭环**

### 1. 学习目标

* 理解为什么要评估 AI 生成内容质量
* 掌握在 Coze Flow 中自动化质量检测的方法
* 能设计客观可量化的质量指标
* 学会构建“评估 → 调整 → 再生成”的闭环
* 将人工反馈与自动评估结合

---

### 2. 课程讲解

#### 2.1 为什么要做质量评估

* **保证交付一致性**：不同时间、不同版本保持相同水准
* **减少人工审核压力**：自动过滤低质量内容
* **快速迭代 Prompt**：知道哪里不好，才能精准优化
* **模型路由优化**：根据历史表现调整模型使用比例

---

#### 2.2 质量评估的三种方法

| 方法                           | 优点      | 缺点     | 适用场景       |
| ---------------------------- | ------- | ------ | ---------- |
| **规则检测**（正则 / 关键词 / 长度）      | 快速、可控   | 无法理解语义 | 格式检测、敏感词过滤 |
| **AI 评审**（用 LLM 评判另一 LLM 输出） | 理解语义，灵活 | 增加调用成本 | 创意质量、逻辑一致性 |
| **人工评分**（飞书/表单）              | 高可信度    | 成本高    | 高价值任务终审    |

---

#### 2.3 核心质量指标（可量化）

1. **完整性**：是否包含必需字段（标题、正文、标签）
2. **可读性**：语言流畅度、逻辑结构
3. **相关性**：与输入主题的匹配度
4. **创新度**：内容是否新颖，不雷同
5. **合规性**：无敏感词、违规内容
6. **执行性**：对任务目标的落地程度

---

### 3. 在 Coze 中的实现步骤

#### 步骤 1：定义评估节点

* 使用 AI 模型作为“评审员”
* Prompt 示例：

  ```
  你是一位内容质检员，请根据以下指标为内容打分（0-10）并给出建议：
  1. 完整性
  2. 可读性
  3. 相关性
  4. 创新度
  5. 合规性
  输出 JSON 格式：
  {
    "完整性": x,
    "可读性": x,
    "相关性": x,
    "创新度": x,
    "合规性": x,
    "综合建议": "..."
  }
  ```

---

#### 步骤 2：设定阈值与处理逻辑

```js
if (完整性 < 7 || 合规性 < 10) → 直接丢弃并重生成;
else if (平均分 < 8) → 自动优化 Prompt 再生成;
else → 进入发布流程;
```

---

#### 步骤 3：人工反馈环节

* 使用飞书交互卡片：

  * 展示 AI 生成结果
  * 提供“👍 通过 / 👎 修改”按钮
  * 可输入修改建议
* 人工反馈数据存入数据库，供后续训练参考

---

#### 步骤 4：闭环优化

1. 收集 AI 评审与人工评分
2. 统计低分内容的共同问题
3. 优化 Prompt 或更换模型
4. 重新测试，记录分数变化
5. 持续迭代

---

### 4. 高级玩法

* **双模评审**：两个不同模型交叉评估，提高可信度
* **自动标签化问题**：给低质量内容打标签（逻辑混乱、内容空洞、违规）
* **自适应 Prompt**：

  * 根据历史评分动态调整 Prompt 强度（如“更具体”“更生动”）
* **权重模型选择**：

  * 高分率高的模型优先调用
  * 长期表现差的模型降低使用频率

---

### 5. 实操案例

**案例目标**
抖音脚本生成质量闭环：

1. AI 生成脚本 → AI 评审打分
2. 如果分数低 → 自动优化 Prompt 再生成
3. 发布前 → 运营人员在飞书卡片打分并给意见
4. 收集 AI + 人工评分，存数据库
5. 每周分析低分案例，更新 Prompt 与模型路由

---

### 6. 课后作业

1. 在你的 Flow 中添加 AI 评审节点
2. 设定质量阈值并自动处理低质量内容
3. 接入飞书交互卡片收集人工反馈
4. 一周后分析低分内容的主要问题
5. 调整 Prompt 并测试改进效果

# 内容优化

## 学习目标
- 掌握提示工程（Prompt Engineering）技巧
- 学会优化脚本质量
- 了解抖音内容的优化策略

## 提示工程基础
### 什么是提示工程
- 定义和重要性
- 在内容生成中的作用
- 与传统编程的区别

### 提示词设计原则
- **明确性**：清晰的指令和要求
- **具体性**：详细的场景描述
- **结构化**：有逻辑的信息组织
- **示例化**：提供参考样例

## 脚本质量优化
### 内容结构优化
- **开头设计**：3秒抓住注意力
- **中间展开**：保持观众兴趣
- **结尾设计**：引导互动和关注

### 语言风格优化
- **口语化表达**：贴近用户习惯
- **情感化语言**：增强感染力
- **节奏感控制**：适合短视频节奏

## 抖音内容优化策略
### 情绪引导技巧
- **好奇心激发**：设置悬念和疑问
- **共鸣点挖掘**：找到用户痛点
- **情感共振**：触发情感反应

### 悬念设置方法
- **问题式开头**："你知道为什么...吗？"
- **对比式悬念**："同样是...，为什么..."
- **揭秘式引导**："今天告诉你一个秘密"

### 引导关注的文案技巧
- **价值承诺**：明确告诉用户能获得什么
- **持续更新**：承诺后续内容
- **互动引导**：鼓励评论和分享

## 高级优化技巧
### A/B测试方法
- 设计对比版本
- 数据收集和分析
- 优化策略调整

### 数据驱动优化
- 关键指标监控
- 用户反馈分析
- 持续改进流程

## 实践练习
### 练习1：提示词优化
- 分析现有提示词问题
- 重新设计优化版本
- 对比生成效果

### 练习2：脚本改写
- 选择一个普通脚本
- 应用优化技巧改写
- 评估改进效果

## 优化工具和资源
- 提示词模板库
- 内容质量评估工具
- 行业优秀案例分析

## 常见问题
- 提示词过于复杂
- 生成内容不稳定
- 优化效果不明显

--------------


## **第十二节 · 生成后的质量评估与自动重试**

### 1. 学习目标

* 理解为什么 AI 输出需要质量评估（Eval）
* 学会在 Flow 中添加自动化质检节点
* 掌握常见的质检标准（结构、敏感词、风格匹配等）
* 能在内容不合格时自动触发重试
* 将质检结果记录下来，用于后续优化 Prompt 与流程

---

### 2. 课程讲解

#### 2.1 为什么要做质量评估

* **避免漏字段、错字段**：保证输出符合 Schema
* **减少低质内容**：敏感词、错别字、风格跑偏
* **节省人工审核时间**：自动拦截不合格结果
* **形成可追溯记录**：分析失败原因，优化模型调用

---

#### 2.2 常见质检标准

| 类型        | 检查项          | 示例                       |
| --------- | ------------ | ------------------------ |
| **结构完整性** | 必须字段是否存在     | title / shots / hashtags |
| **内容安全**  | 是否包含敏感词、违规信息 | 禁用词表匹配                   |
| **风格匹配**  | 语气是否符合设定     | 搞笑 / 专业 / 故事化            |
| **事实性**   | 是否引用了 KB 内容  | 引文字段非空                   |
| **长度要求**  | 标题不超过 20 字   | `len(title) <= 20`       |

---

#### 2.3 在 Coze Flow 中实现质检

1. **后置 LLM 检查**（柔性判断）

   * 让一个小模型检查输出质量
   * 提示词示例：

     ```
     你是一个严格的内容审核员。检查以下 JSON 是否符合规则：
     规则：
     1. 必须包含 title, hook, shots, hashtags
     2. title 字数 <= 20
     3. 不能包含敏感词列表：{{banned_words}}
     输出：
     {
       "pass": true/false,
       "issues": ["问题1", "问题2"]
     }
     ```

2. **Schema 校验**（硬性判断）

   * 节点中开启 Output Schema
   * 结构错误 → 自动报错

3. **正则 / 规则节点**

   * 使用表达式节点判断：

     * `len(title) <= 20`
     * `not any(word in title for word in banned_words)`

---

### 3. 步骤演示（自动重试）

#### 目标

脚本生成后，自动检查合格性，不合格时重试一次。

---

**步骤 1：生成节点（LLM）**

* 生成脚本 JSON
* 变量名：`script_json`

---

**步骤 2：质检节点（LLM）**

* 模型：`gpt-3.5-turbo`（省钱快速）
* 输入：`{{script_json}}` + 审核规则
* 输出变量：`check_result`
* 输出 JSON：

  ```json
  {
    "pass": true,
    "issues": []
  }
  ```

---

**步骤 3：条件分支**

* 条件：`check_result.pass == true` → 直接输出
* 否则 → 进入重试分支

---

**步骤 4：重试生成**

* 重新调用 LLM 节点
* 在 Prompt 中加入：

  ```
  请修正以下问题后重新生成：
  {{check_result.issues}}
  ```
* 输出到 `script_json_retry`

---

**步骤 5：合并输出**

* 如果有 `script_json_retry` → 用它作为最终输出
* 否则 → 用初次生成的 `script_json`

---

**步骤 6：记录质检日志**

* 在日志节点中保存：

  * 输入参数
  * 初次生成结果
  * 质检问题
  * 是否触发重试
  * 最终结果

---

### 4. 最佳实践

* **分级处理**：严重问题（敏感词）直接拒绝，轻微问题（长度超标）可自动修正
* **敏感词表动态更新**：从运营部门同步最新违规词
* **统计问题分布**：找出常见错误，优化 Prompt 或 KB
* **限制重试次数**：建议 1\~2 次，防止死循环

---

### 5. 实操案例

**案例目标**
为“多平台文案生成器”添加质检机制：

* 检查每个平台的标题长度、标签数量、是否含敏感词
* 不合格时自动修正

**输入示例**：

```json
{
  "product": "夏季防晒帽",
  "platform": "douyin",
  "style": "funny"
}
```

**质检逻辑**：

1. LLM 审核规则
2. 条件判断分支
3. 自动修正并重试一次
4. 保存质检结果到飞书表格

---

### 6. 课后作业

1. 在你的脚本生成 Flow 中添加质检节点
2. 检查：

   * 必须字段是否存在
   * 标题长度是否 ≤ 20
   * 是否包含敏感词表中的任一词
3. 不合格时自动重试一次
4. 记录至少 5 次运行的质检日志
5. 写一段总结：常见问题类型与下一步优化方案


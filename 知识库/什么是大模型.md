## 1. 核心定义

* **规模**：“大”主要体现在参数数量（亿、百亿甚至千亿级别）和训练数据量（TB 级甚至 PB 级）
* **通用性**：并非为某个单一任务训练，而是具备跨领域、跨任务的迁移能力
* **多功能**：支持文本、代码、图像、音频、视频等多种输入输出形式（多模态）

> 例子：GPT-4、Claude、文心一言、通义千问、Gemini、LLaMA 等

---

## 2. 核心组成

| 组件       | 说明                  | 示例              |
| -------- | ------------------- | --------------- |
| **模型架构** | 多层 Transformer 网络为主 | GPT、BERT、T5     |
| **训练数据** | 涵盖通用文本、代码、图像等       | 互联网数据、开源语料、专有语料 |
| **参数规模** | 决定模型容量与复杂度          | 百亿\~千亿级参数       |
| **推理引擎** | 支持模型快速运行与推理         | GPU、TPU、推理框架    |
| **微调机制** | 适配特定业务场景            | LoRA、指令微调、RAG   |

---

## 3. 核心能力

1. **自然语言理解**（NLU）：阅读理解、情感分析、信息提取
2. **自然语言生成**（NLG）：写文章、生成脚本、翻译
3. **推理与规划**：多步推理、链式思考（Chain of Thought）
4. **代码生成与调试**：自动补全、代码修复
5. **多模态处理**：图像理解、语音识别、视频内容生成
6. **知识检索与问答**：结合外部知识库进行精准回答

---

## 4. 训练与优化方式

* **预训练（Pre-training）**：在大规模通用语料上训练，学会语言模式
* **指令微调（Instruction Tuning）**：让模型按人类指令做事
* **人类反馈强化学习（RLHF）**：让模型的回答更贴近人类偏好
* **增量训练 / LoRA**：低成本适配垂直领域
* **RAG（检索增强生成）**：调用外部知识提升准确性

---

## 5. 应用场景

* **通用对话助手**（ChatGPT、Claude）
* **智能客服与知识问答**
* **内容创作**（文章、短视频脚本、营销文案）
* **编程辅助**（GitHub Copilot、Cursor）
* **数据分析与BI报表生成**
* **多模态应用**（图像生成、视频编辑、语音合成）

---

## 6. 大模型与传统模型的区别

| 对比维度     | 大模型          | 传统模型       |
| -------- | ------------ | ---------- |
| **数据规模** | TB\~PB 级     | MB\~GB 级   |
| **任务范围** | 跨任务通用        | 针对单一任务     |
| **迁移能力** | 强，可零样本/少样本学习 | 弱，需要大量标注数据 |
| **开发门槛** | 高，需算力资源      | 较低         |
| **应用形态** | API 调用/平台部署  | 嵌入式或定制部署   |

